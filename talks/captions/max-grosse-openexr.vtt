WEBVTT

1
00:07.200 --> 00:10.360
Welcome to my presentation on Reviewing production

2
00:10.360 --> 00:13.240
OpenEXR files on the web for Machine Learning.

3
00:13.240 --> 00:15.680
I'm Max Grosse, Principal Software Engineer at

4
00:15.680 --> 00:17.480
DisneyResearch|Studios.

6-p
00:17.480 --> 00:18.960
I will show you a little bit of our

7
00:18.960 --> 00:20.800
machine learning pipeline we employ here,

8
00:20.800 --> 00:23.160
and how dealing with feature film production assets

9
00:23.160 --> 00:24.960
provides a unique challenge in this,

10
00:24.960 --> 00:29.800
and how we address it using modern web technology.

slide-2
00:29.840 --> 00:31.480
As we are part of the Studio segment of

13
00:31.480 --> 00:33.280
the Walt Disney Company, we naturally

14
00:33.280 --> 00:35.120
deal with production assets.

15-p
00:35.120 --> 00:36.760
When it comes to images, this typically

16
00:36.760 --> 00:38.320
means we deal with higher resolution,

17
00:38.320 --> 00:41.800
1080p at least, up to 4k and even more.

18
00:41.800 --> 00:44.000
Additionally, the imagery is typically

19
00:44.000 --> 00:45.800
of high dynamic range, stored in the

20
00:45.800 --> 00:48.720
OpenEXR file format as 16-bit or 32-bit

21
00:48.720 --> 00:50.720
floating points.

23-p
00:50.720 --> 00:52.480
For many applications, we also need to

24
00:52.480 --> 00:54.600
inspect other aspects of images

25
00:54.600 --> 00:57.400
apart from the final composed color data.

26
00:57.400 --> 00:58.960
These can be feature buffers from rendering,

27
00:58.960 --> 01:01.480
such as depth, normals, or alpha masks

28
01:01.480 --> 01:03.960
and provided as separate inputs so you can

29
01:03.960 --> 01:06.360
quickly determine for each final color pixel

30
01:06.360 --> 01:08.640
the depth or normal that goes along with it.

31-p
01:08.640 --> 01:11.920
To inspect the training data that

33
01:11.920 --> 01:14.440
gets fed into our deep neural networks,

34
01:14.440 --> 01:17.120
to view error maps on the validation,

35
01:17.120 --> 01:19.280
and generally being able to judge

36
01:19.280 --> 01:21.400
the visual quality of our results,

37
01:21.400 --> 01:24.280
or enable artists to properly judge these,

38
01:24.280 --> 01:25.920
we require a little bit more than

39
01:25.920 --> 01:28.200
a cut-out thumbnail of a jpeg image within,

40
01:28.200 --> 01:30.400
for example, Tensorboard.

slide-3
01:31.200 --> 01:35.720
An example of our work is the Kernel-Predicting

43
01:35.720 --> 01:37.960
Convolutional Networks for Denoising Monte

44
01:37.960 --> 01:40.360
Carlo Renderings that my colleagues published

45
01:40.360 --> 01:42.800
in 2017 at ACM SIGGRAPH.

47-p
01:43.640 --> 01:46.120
This introduces a deep learning approach for

48
01:46.120 --> 01:48.800
denoising Monte-Carlo rendered images that

49
01:48.800 --> 01:51.200
produces high-quality results suitable for production.

slide-4
01:52.560 --> 01:54.400
This is when work on JERI started,

52
01:54.400 --> 01:56.920
the JavaScript Extended-Range image viewer.

54-p
01:56.920 --> 02:00.760
The idea is that we can have a remote,

55
02:00.760 --> 02:03.600
or local, HTTP server that serves not only

56
02:03.600 --> 02:06.880
the OpenEXR images we are interested in directly,

57
02:06.880 --> 02:09.640
but also an HTML5-based viewer such that

58
02:09.640 --> 02:11.640
these OpenEXR images can be directly

59
02:11.640 --> 02:15.800
inspected within a modern browser.

61-p
02:15.800 --> 02:16.920
In particular, this allows conveniently

62
02:16.920 --> 02:19.400
reviewing result images stored on

63
02:19.400 --> 02:20.880
our compute cluster without the need

64
02:20.880 --> 02:22.960
to explicitly copy or mount it locally.

66-p
02:23.480 --> 02:25.320
This also guarantees that the client

67
02:25.320 --> 02:26.880
receives the original images,

68
02:26.880 --> 02:28.520
without any additional compression,

69
02:28.520 --> 02:30.600
providing full control over the very exact

70
02:30.600 --> 02:32.760
pixel values that will be displayed.

72
02:33.160 --> 02:34.680
It functions like a very configurable

73
02:34.680 --> 02:37.800
&lt;img&gt; tag with a lot of additional features

74
02:37.800 --> 02:39.720
to dig deep into extended-range images.

slide-5
02:41.920 --> 02:43.880
For this, we have compiled the OpenEXR

77
02:43.880 --> 02:46.760
library to WebAssembly using EMScripten.

79-p
02:46.760 --> 02:49.120
The EMScripten toolchain still lacks a bit

80
02:49.120 --> 02:50.400
in terms of quality of life,

81
02:50.400 --> 02:52.240
so this was a bit of a tedious process

82
02:52.240 --> 02:54.520
to get right, however once built,

83
02:54.520 --> 02:55.880
it can be potentially used in

84
02:55.880 --> 02:57.520
a variety of applications.

86-p
02:57.520 --> 03:01.120
Decoding speed was not our primary concern,

87
03:01.120 --> 03:03.880
so we did not perform any extensive benchmarks

88
03:03.880 --> 03:06.240
on how fast it can decode EXR images

89
03:06.240 --> 03:07.720
compared to a native solution.

90
03:07.720 --> 03:10.320
Typically, even on local network,

91
03:10.320 --> 03:12.120
we will already have a small delay for

92
03:12.120 --> 03:15.280
downloading the EXR, so decoding speed did not

93
03:15.280 --> 03:17.720
show to be an issue to our users so far.

95-p
03:18.600 --> 03:20.800
In particular with some local caching,

96
03:20.800 --> 03:22.720
switching the loaded images usually

97
03:22.720 --> 03:24.280
is instantaneous anyway.

slide-6
03:26.400 --> 03:28.480
Given the extended range nature of

100
03:28.480 --> 03:31.360
the input imagery, we need a way to control things

101
03:31.360 --> 03:33.880
like gamma and exposure, also in case we want to

102
03:33.880 --> 03:37.160
drill into details at particular dark or bright regions.

104-p
03:37.160 --> 03:41.640
Deep learning is commonly driven by a loss function,

105
03:41.640 --> 03:44.240
which in our case is often a combination of

106
03:44.240 --> 03:47.320
image difference metrics. Visualizing this error

107
03:47.320 --> 03:48.920
is of great importance in helping

108
03:48.920 --> 03:50.400
develop the desired model.

110-p
03:50.400 --> 03:53.320
For all these visualization aspects,

111
03:53.320 --> 03:55.560
we have opted to facilitate WebGL.

112
03:55.560 --> 03:57.800
This was a very natural choice and provides

113
03:57.800 --> 03:59.600
a very efficient and convenient way to

114
03:59.600 --> 04:03.400
change the way things get displayed without too much code

115
04:03.400 --> 04:06.120
and without modifying the original pixel values directly.

117-p
04:06.120 --> 04:09.400
In particular, this offloads all the pixel operations

118
04:09.400 --> 04:11.800
to the GPU, keeping the user experience smooth

119
04:11.800 --> 04:14.360
and avoiding manipulating large arrays of pixels

120
04:14.360 --> 04:16.640
directly in a single JavaScript thread.

122-p
04:16.640 --> 04:21.800
The basic viewer application then was written

123
04:21.800 --> 04:23.520
in mostly TypeScript with React.js, optionally

124
04:23.520 --> 04:26.280
handling the UI aspects and helping integrating

125
04:26.280 --> 04:28.680
the viewer into other React.js projects.

slide-7
04:28.680 --> 04:32.880
The viewer itself is configured by providing

128
04:32.880 --> 04:36.680
a JSON file that describes which EXR images to load,

129
04:36.680 --> 04:39.960
the remote paths to find them, which images to group,

130
04:39.960 --> 04:43.000
as well which images together should form an image difference error map

131
04:43.000 --> 04:44.800
(i.e. image differences).

slide-8
04:44.800 --> 04:51.640
Here's a quick demonstration of JERI in action.

135-p
04:51.640 --> 04:54.920
The examples are right from the jeri website

136
04:54.920 --> 04:57.800
and only intended to illustrate the idea,

137
04:57.800 --> 04:59.160
note that these are not production assets and

138
04:59.160 --> 05:02.000
the results are far worse than our production

139
05:02.000 --> 05:05.920
results and serve only as illustration.

141-p
05:05.920 --> 05:09.840
In this example, we have a noisy input

142
05:09.840 --> 05:12.240
that we might have gotten from our renderer.

144-p
05:12.280 --> 05:14.880
We can now toggle to "Improved" to inspect

145
05:14.880 --> 05:17.760
the results from a simple denoiser.

147-p
05:17.760 --> 05:23.680
Then We can zoom in and pan around

148
05:23.680 --> 05:30.920
to really compare even on a pixel level.

150-p
05:30.960 --> 05:39.840
Ultimately, we are interested in

151
05:39.840 --> 05:41.480
how we compare to a ground truth,

152
05:41.480 --> 05:43.960
or reference, which is also available here

153
05:43.960 --> 05:48.320
and we can simply toggle to that as well.

155-p
05:48.320 --> 05:50.360
As you see for example there's quite some detail

156
05:50.360 --> 05:52.800
lost in the ice cubes.

158-p
05:52.800 --> 05:53.800
Even better though is to have an error map

159
05:53.800 --> 05:56.800
between the output and the reference

160
05:56.800 --> 05:59.600
computed on the fly, as you can see here.

162-p
05:59.600 --> 06:02.200
Here we can also adjust the exposure,

163
06:02.200 --> 06:04.120
so we can better visualize parts

164
06:04.120 --> 06:06.920
we could otherwise not see in that detail.

166-p
06:06.920 --> 06:12.280
Of course the exposure adjustment work

167
06:12.280 --> 06:14.280
for color images just the same.

169-p
06:14.280 --> 06:19.720
We can repeat that for a different set of images,

170
06:19.720 --> 06:22.680
if we want to see how it performed on different input.

slide-9
06:22.680 --> 06:29.400
We have integrated it into our ML monitoring system

173
06:29.400 --> 06:31.160
that is running on our cluster.

175-p
06:31.200 --> 06:33.320
Here you can see a more typical use case

176
06:33.320 --> 06:35.400
where you can see the recorded training

177
06:35.400 --> 06:37.160
runs on the left, and a lot of different

178
06:37.160 --> 06:39.600
sets of images and metrics displayed in

179
06:39.600 --> 06:41.680
the main plane, allowing to quickly

180
06:41.680 --> 06:44.280
drill down and monitor your progress and results.

182-p
06:44.280 --> 06:47.600
For example we can look at different

183
06:47.600 --> 06:49.920
validation images, different channel sets

184
06:49.920 --> 06:52.400
and at different points in time during the training.

185
06:52.400 --> 06:55.320
At any time we could also fire up a Tensorboard

186
06:55.320 --> 06:57.640
to look at other recorded metrics or

187
06:57.640 --> 07:00.640
write a report on our results, so a web-based

188
07:00.640 --> 07:03.280
solution like JERI really fits in nicely.

slide-10
07:03.320 --> 07:09.240
We have released JERI as Open Source Software,

191
07:09.240 --> 07:12.400
you can try it out yourself and integrate it

192
07:12.400 --> 07:16.360
into your own solutions, see jeri.io for details.

slide-11
07:16.360 --> 07:20.800
That's all from my side.

195
07:20.800 --> 07:25.400
Thanks you for your attention.

