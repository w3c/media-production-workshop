WEBVTT

1
00:07.400 --> 00:08.400
<v ->Hello, there.</v>

2
00:08.400 --> 00:09.150
My name is Sacha Guddoy

3
00:09.150 --> 00:12.100
and I'm the lead Front End Engineer at Grabyo.

slide-2
00:12.100 --> 00:15.170
Grabyo is a SaaS platform which delivers tools

5
00:15.170 --> 00:19.280
for live broadcast production to commercial broadcasters.

6-p
00:19.280 --> 00:22.470
Some of our offerings include live broadcast production,

7
00:22.470 --> 00:25.450
video editing, clipping from live streams,

8
00:25.450 --> 00:28.203
and publishing to various endpoints.

slide-3
00:30.490 --> 00:34.630
At Grabyo, we use WebRTC in our live production offering.

10-p
00:35.100 --> 00:37.910
The way that this works is the user will see,

11
00:37.910 --> 00:38.880
in their web browser,

12
00:38.880 --> 00:41.650
they'll have multiple live streams coming in

13
00:41.650 --> 00:44.810
and they will be able to monitor these live streams

14
00:44.810 --> 00:47.976
and choose which ones are being output

15
00:47.976 --> 00:50.363
to their broadcast endpoint.

16-p
00:52.150 --> 00:54.750
We also have multiple sidecar applications

17
00:54.750 --> 00:56.240
and multi-window workflows.

18
00:56.240 --> 00:58.720
For example, popping out a player.

slide-4
00:58.720 --> 01:00.440
One of the challenges that we face is

20
01:00.440 --> 01:02.310
the synchronization of streams.

21-p
01:02.310 --> 01:06.620
What we'd like to do is to have multiple live feeds

22
01:06.620 --> 01:08.000
from different cameras coming in

23
01:08.000 --> 01:10.170
and to be able to switch between them.

24
01:10.170 --> 01:12.560
But if our live feeds aren't perfectly in sync

25
01:12.560 --> 01:13.393
with each other,

26
01:13.393 --> 01:15.180
if those two cameras aren't perfectly in sync,

27
01:15.180 --> 01:16.880
it's going to be very noticeable when

28
01:16.880 --> 01:18.550
you switch between them that there's some delay

29
01:18.550 --> 01:21.730
between them and it's jarring for the viewer.

30-p
01:21.730 --> 01:24.960
When you have multiple WebRTC streams on your page,

31
01:24.960 --> 01:27.140
keeping those all in sync is not necessarily

32
01:27.140 --> 01:29.810
the most straightforward thing.

33
01:29.810 --> 01:33.833
The browser will do its best, but they aren't tied together.

34-p
01:35.490 --> 01:36.323
So, for example,

35
01:36.323 --> 01:40.260
if you are cutting between different cameras,

36
01:40.260 --> 01:43.850
you want those camera feeds to be showing exactly

37
01:43.850 --> 01:45.410
at the same time.

38
01:45.410 --> 01:48.930
If you're doing multi-party chat, you don't want latency.

slide-5
01:48.930 --> 01:51.600
The synchronization aspect is pretty difficult.

40
01:51.600 --> 01:53.700
Network conditions can be unpredictable

41
01:55.280 --> 01:58.000
and you don't really have a way of correcting for that

42
01:58.000 --> 02:00.750
or to reconcile with synchronization of streams

43
02:00.750 --> 02:01.700
on the client side.

44-p
02:02.760 --> 02:05.420
If there were embedded time stamps on the streams,

45
02:05.420 --> 02:07.970
then you potentially could do that.

46
02:07.970 --> 02:10.870
Using something lower level, such as Web Transport,

47
02:10.870 --> 02:12.140
may allow you to do that

48
02:12.140 --> 02:15.990
and may even be a more performant technology

49
02:15.990 --> 02:18.263
for this use case than WebRTC anyway.

slide-6
02:19.620 --> 02:22.400
One pattern that we've been using recently is

51
02:22.400 --> 02:25.900
splitting workflows into different browser contexts.

52
02:25.900 --> 02:29.220
Being able to create a pop out window

53
02:29.220 --> 02:31.150
which allows you to monitor

54
02:31.150 --> 02:34.850
a specific video in one window

55
02:34.850 --> 02:38.370
and be able to monitor everything else in another window.

56-p
02:38.370 --> 02:40.750
Or to be able to edit your audio in one window

57
02:40.750 --> 02:42.850
and monitor your videos in another window.

58
02:43.750 --> 02:47.300
In that last scenario, you're going to be having

59
02:47.300 --> 02:49.500
two instances in your browser

60
02:49.500 --> 02:51.260
of that same WebRTC connection.

61
02:51.260 --> 02:54.380
If I wanted to have the video of my live stream

62
02:54.380 --> 02:58.000
in one window because that's my video control suite

63
02:58.000 --> 03:01.710
and I want to have the same live stream in another window

64
03:01.710 --> 03:03.860
because that's my audio control suite,

65
03:03.860 --> 03:06.200
then I have to have two WebRTC connections.

66
03:06.200 --> 03:07.940
That's twice the performance overhead,

67
03:07.940 --> 03:09.693
twice the bandwidth, et cetera.

68-p
03:10.870 --> 03:13.880
We think about the way that Shared WebWorkers work,

69
03:13.880 --> 03:16.120
the SharedWorker itnerface,

70
03:16.120 --> 03:20.490
that allows multiple contexts to share whatever

71
03:20.490 --> 03:22.660
is happening in that Worker.

72
03:22.660 --> 03:25.570
If we could do the exact same thing with WebRTC,

73
03:25.570 --> 03:28.420
that would significantly reduce our performance overhead.

74-p
03:30.340 --> 03:32.290
And these kinds of workflows are really powerful

75
03:32.290 --> 03:35.360
for professional desktop applications.

76
03:35.360 --> 03:38.500
If you are a video editor using some kind of NLE,

77
03:38.500 --> 03:39.880
you probably want as much screen space as you want

78
03:39.880 --> 03:41.760
for your timeline, your monitors,

79
03:41.760 --> 03:44.400
your asset bins, et cetera.

80
03:44.400 --> 03:46.980
Being able to kind of split different parts

81
03:46.980 --> 03:49.100
of our interfaces out into different windows

82
03:49.100 --> 03:51.400
so the user can position them as they see fit

83
03:51.400 --> 03:52.340
is really, really helpful.

slide-7
03:53.200 --> 03:55.950
What are the advantages of doing this?

85-p
03:55.950 --> 03:57.810
There's obviously less resource consumption

86
03:57.810 --> 04:00.263
because you only have that one connection.

87-p
04:01.760 --> 04:04.870
There's inherent synchronization between the contexts

88
04:04.870 --> 04:08.503
because the data is coming from that same connection.

89
04:09.820 --> 04:12.900
Now this is probably possible using shared workers

90
04:12.900 --> 04:14.730
and Web Transport.

91
04:14.920 --> 04:17.833
But browser support for that is not particularly great.

92-p
04:18.840 --> 04:21.323
Accuracy is also important in this technology.

93
04:22.640 --> 04:25.580
More accurate time stamps might help us

94
04:25.580 --> 04:27.450
synchronize those streams together.

95
04:27.450 --> 04:31.220
And also it helps synchronize other things.

96-p
04:31.220 --> 04:34.840
For example, synchronize an overlay in the DOM.

97
04:34.840 --> 04:36.373
Or a notification in the DOM.

slide-8
04:37.280 --> 04:39.730
Capability to encode and decode data

99
04:39.730 --> 04:42.860
from a WebRTC connection would also be really useful.

100-p
04:42.860 --> 04:44.460
Right now, the API surface of

101
04:44.460 --> 04:48.200
the WebRTC connection is pretty minimal

102
04:48.200 --> 04:51.593
and it doesn't expose much useful information to us.

103
04:52.890 --> 04:57.217
Being able to put our own code in that pipeline

104
04:57.217 --> 05:00.200
would allow us to do all this interesting stuff.

105-p
05:00.200 --> 05:03.460
Say, for example, presenting a particular frame

106
05:03.460 --> 05:04.810
when we want to present it.

107
05:04.810 --> 05:07.640
Say, for example, synchronizing audio and video

108
05:07.640 --> 05:09.840
from different browser windows.

109-p
05:09.840 --> 05:13.480
We could know exactly which frame is being presented

110
05:13.480 --> 05:15.290
before they even get rendered to the DOM,

111
05:15.290 --> 05:17.780
so we can prepare our DOM elements

112
05:17.780 --> 05:19.880
which would synchronize to that.

113
05:19.880 --> 05:21.470
We could potentially send over

114
05:21.470 --> 05:23.770
proprietary error correction data

115
05:23.770 --> 05:26.350
to smooth over any link failures

116
05:26.350 --> 05:28.473
with picture quality as a priority.

117-p
05:29.770 --> 05:31.700
And going back the other way,

118
05:31.700 --> 05:32.800
you could do stuff like funny hats.

119
05:32.800 --> 05:33.810
You could do chroma keying.

120
05:33.810 --> 05:35.860
You could do machine learning analysis

121
05:35.860 --> 05:37.410
and do stuff like background blur

122
05:37.410 --> 05:40.240
or embedding metadata.

123-p
05:40.240 --> 05:42.610
A lot of this can be solved using

124
05:42.610 --> 05:46.830
the MediaStreamTrack Insertable Streams feature.

125
05:47.200 --> 05:49.100
That is still in a draft specification

126
05:49.950 --> 05:53.393
and I'd really love to see more browser support for that.

127-p
05:55.810 --> 05:56.643
Thank you for watching.

128
05:56.643 --> 05:58.470
I hope you enjoyed hearing about our use cases

129
05:58.470 --> 05:59.940
and I'm looking forward to hearing

130
05:59.940 --> 06:02.240
any questions and feedback.

131
06:02.240 --> 06:03.730
Thanks, bye.
