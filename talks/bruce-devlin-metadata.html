<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Metadata in Production Workflows - Bruce Devlin - W3C/SMPTE Joint Workshop on Professional Media Production on the Web</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../style.css">
    <link rel="stylesheet" href="../talk-ui.css">
    <link rel="stylesheet" href="https://www.w3.org/2019/09/TPAC-template/fonts/iconmonstr-iconic-font.css">
    <meta name="twitter:site" content="@w3c">
    <meta name="twitter:card" content="player">
    <meta property="og:title" content="Metadata in Production Workflows by Bruce Devlin (SMPTE)">
    <meta property="og:description" content="There are many interoperable solutions for video and audio pipelines that start or terminate in a browser and incorporate ingest, playback, editing and other workflows. When it comes to production metadata associated with video and audio, it&apos;s much more of a mess.

Metadata might be standardised like a GPS coordinate or time of day in ISO 8601 format. It might be proprietary like a tripod&apos;s gimbal position or characteristics of on-set lighting. Metadata might be bursty, slow and not synchronised to the video like a log of who&apos;s in shot or it might be high speed, binary and related to free-running clock like lens position or zoom setting. Metadata might be generated by a device like the velocity of a car, buggy, dolly or crane. Metadata might be derived from on-set stimuli such as the LED wall virtual set velocity.

The only common factor to all of this metadata is that none of it makes it to the VFX / Editor / Mastering suite unless you do a bunch of customs stuff in your workflow. I think we can do better. What do you think?">
    <meta property="og:image" content="thumbnails/bruce-devlin-metadata.jpg">
    <meta property="og:video" content="https://iframe.videodelivery.net/0e9a8d4f784363124c5b9efcc5ddd015">
    <meta property="twitter:player" content="https://iframe.videodelivery.net/0e9a8d4f784363124c5b9efcc5ddd015?defaultTextTrack=en">
    <meta property="twitter:player:width" content="360">
    <meta property="twitter:player:height" content="202">
  </head>
  <body>
    <form id=form><!-- form to request a page in kiosk mode --></form>
    <header id="header" class="header">
      <div id="banner">
        <div>
          <p>
            <a href="https://www.w3.org/"><img alt="W3C" src=
            "../media/w3c_home_nb-v.svg" height="48" width="72"></a>
            <a href="https://www.smpte.org/"><img alt="SMPTE" src=
            "../media/smpte_logo.png" height="48"></a>
          </p>
          <div class="banner-title">
            <h1>
              W3C/SMPTE Joint Workshop on Professional Media Production on the Web
            </h1>
          </div>
          <p class="attribution">
            <span>Timeline photo by <a href="https://unsplash.com/@kineticbear?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Jacob Miller</a> on <a href="https://unsplash.com/s/photos/timeline?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>
          </p>
          <p>9-18 November 2021; online event</p>
        </div>
      </div>
      <nav class="menu" id="menu">
        <ul>
          <li>
            <a href="../">Call for Participation</a>
          </li>
          <li>
            <a href="../talks.html">Talks</a>
          </li>
          <li>
            <a href="../speakers.html">Apply as a speaker</a>
          </li>
        </ul>
      </nav>
    </header>
    <main id="main" class="main talk">
      <section id="intro">
        <h2>Metadata in Production Workflows</h2>

        <p class="talkinfo">
          Presenter: <strong>Bruce Devlin (SMPTE)</strong><br>
          Duration: <strong>6 minutes</strong><br>
          Slides: <a href="null"><strong>PDF</strong></a>
        </p>

        <p class="buttons">
          <!-- previousStart -->
          <button form="form" type="submit" class="picto im-angle-left" formaction="steve-cronan-production-metaverse.html">
            Previous: The Production Metaverse
          </button>
          <!-- previousEnd -->
          <a href="../talks.html" class="picto im-data">All talks</a>
          <!-- nextStart -->
          <button form="form" type="submit" formaction="julian-fernandez-campon-ipaas.html">
            Next: The Power of an iPaaS for Media<span class="picto im-angle-right"></span>
          </button>
          <!-- nextEnd -->
        </p>
      </section>

      <section id="talk">
        <h3 style="display:none">Slides &amp; video</h3>

        <details>
          <summary>Keyboard shortcuts in the video player</summary>
          <ul>
            <li>Play/pause: <kbd>space</kbd>
            </li><li>Increase volume: <kbd>up arrow</kbd>
            </li><li>Decrease volume: <kbd>down arrow</kbd>
            </li><li>Seek forward: <kbd>right arrow</kbd>
            </li><li>Seek backward: <kbd>left arrow</kbd>
            </li><li>Captions on/off: <kbd>C</kbd>
            </li><li>Fullscreen on/off: <kbd>F</kbd>
            </li><li>Mute/unmute: <kbd>M</kbd>
            </li><li>Seek to 0%, 10%â€¦ 90%: <kbd>0-9</kbd>
          </li></ul>
        </details>

        <div id="player">
          <script type="application/ld+json">
            {
              "@context": "https://schema.org",
              "@type": "VideoObject",
              "name": "Metadata in Production Workflows",
              "description": "There are many interoperable solutions for video and audio pipelines that start or terminate in a browser and incorporate ingest, playback, editing and other workflows. When it comes to production metadata associated with video and audio, it's much more of a mess.\n\nMetadata might be standardised like a GPS coordinate or time of day in ISO 8601 format. It might be proprietary like a tripod's gimbal position or characteristics of on-set lighting. Metadata might be bursty, slow and not synchronised to the video like a log of who's in shot or it might be high speed, binary and related to free-running clock like lens position or zoom setting. Metadata might be generated by a device like the velocity of a car, buggy, dolly or crane. Metadata might be derived from on-set stimuli such as the LED wall virtual set velocity.\n\nThe only common factor to all of this metadata is that none of it makes it to the VFX / Editor / Mastering suite unless you do a bunch of customs stuff in your workflow. I think we can do better. What do you think?",
              "thumbnailUrl": "thumbnails/bruce-devlin-metadata.jpg",
              "duration": "PT5M58S",
              "embedUrl": "https://iframe.videodelivery.net/0e9a8d4f784363124c5b9efcc5ddd015",
            }
          </script>

          <iframe id="video" title="Metadata in Production Workflows" src="https://iframe.videodelivery.net/0e9a8d4f784363124c5b9efcc5ddd015"
            allow="accelerometer; autoplay; encrypted-media; picture-in-picture"
            allowfullscreen="" width="640" height="360" frameborder="0"></iframe>

          <div id="slides" class="fade-in" role="region" aria-live="off" aria-label="Slide container">
            <div>
  <p>So, you want to watch video on the web. There are a bunch of standard APIs and protocols for getting moving images and sound into your browser. Sure, some of them are super high latency, and super high compression, while others are super low compression and super low latency. But by and large, you can pick an operating point and find enough tooling to get, er, 90 to 100 percent of the job done for you.</p>
  <p>Now, you decide to go the other way. You want to ingest some video. Again, there are a bunch of APIs and protocols that will allow you to, well, vary the compression ratios, and the latency, and the quality, and the reliability, and the speed, and the realtime-ness, and a bunch of other things based on your needs, to get an ingest working. It's not quite as slick, but it works.</p>
  <p>Now, you want to add some production metadata to the ingest and get it into your cloud production system. I normally class this sort of metadata as exotic", because there are, well, loads of different types, and each type has typically got a tiny little usage, and there are very few really good infrastructures or frameworks for solving the general case. I've seen mixes of Arduino and Raspberry Pi, with RTP feeds over WiFi, with custom rate control, and varying file types, different ways of time... portrayal, and different synchronizations, different error corrections, and, meanwhile, the common problem remains the same.</p>
  <p>I'd like to get this metadata from that thing, over a timeline, onto my video and audio, while I'm shooting, and I want to be able to figure out which metadata, from which device, was associated with which take, from which camera, during the shoot. Simple, huh?</p>
  <p>Well, none of this is really new. We've been bundling these types of systems together since the early days of cinema and television. What's changed is that we now have the ability to pull vast quantities of data into a common cloudy store, and to play as much compute, and as much... stuff, as you can afford in the Cloud. The wonderful folks at Arri, Nablet, and TrackMen helped put together some kit to explore the outline of a generic solution, and we documented that piece of work on the website mxf-live.io. You can see that we captured some data from the inside of the Arri camera, as well as an external data feed, with the pan, the tilt, and the oar from the tripod head, encapsulated it into a standard file format, we stuck it across the WiFi, and then serialized it onto the editor's timeline, so that we could do post-production on the live feed, with the captured metadata. Great! Job done.</p>
  <p>Not quite. We figured out that there are really four major sorts of metadata, split along two axes. The first axis is pretty simple. Is it binary, or is it text? And this is important, 'cause that turns out to be a good predictor of how you're gonna process the metadata downstream. The second axis is where the data is isochronous. In other words, one sample for each clock tick, every clock tick, or is it kinda lumpy, with embedded timing?</p>
  <p>Let's look an example of metadata for each of those quadrants. Isochronous binary is pretty common. That's the sort of thing that you'd find the position of the lens, sampled every chirp of its piezoelectric motor, or maybe, sampled every frame. Isochronous text, this might be, oh, all the Dolby Vision high dynamic range metadata property, stored as an XML document for every frame of the video. Blobs of binary data, this might be an event track, signaling which smoke machine was turned on, and at what time of day. Blobs of text data, well, this is pretty common. That's exactly what closed captioning and subtitles are, where each phrase of text is as labeled, with the timing information for that phrase.</p>
  <p>Once you've got these four fundamental types, you can then look at transport. It helps to be able to store this timed data as a serializable stream of packets, and for that we chose MXF, because of the available infrastructure, and the ability to represent clocks as rational numbers, so the precise timing can be maintained without the risk of drift if the systems are left running for long periods of time, like weeks, and months, and years. Those packets can then be mapped and layered with different transports, like WebRTC, to get them from where they are, to where they need to be.</p>
  <p>And now, it starts to get interesting. Whilst MXF is pretty handy for the hardware and firmware layers, there are open source projects, like OpenTimelineIO from Pixar, and the ASWS, that are much more friendly for interacting with that data in a generic, product-independent way. Having said that, we know that transcoding video is lossy, and transcoding metadata may actually destroy its usefulness. So, wherever possible, retaining the original, possibly bulky, metadata in its original form, is vital, and computing simplified proxies for that metadata becomes important for visualization. If you're going to do all of that in the general case, then managing identifiers of the metadata type, and how it's associated with other assets, could become a complexity nightmare, unless you design some sort of common framework for associating elements, based on simple identifiers, such as URIs.</p>
  <p>And that's as far as we've got. There's interest from studios, and vendors, and a whole bunch of people, and I hope to put more time into it, personally, when I pass on my role as SMPTE Standards Vice President to someone else in January, and if it interests you, then I'd love to talk. I'm sure there is something here that the brain trust of W3C and SMPTE can genuinely create something that's useful for the world of media, as well as for other verticals. Thanks!</p>
</div>


          </div>
        </div>
      </section>

      <section id="extrabuttons">
        <p class="buttons">
          <!-- previousStart -->
          <button form="form" id="prevtalk" type="submit" formaction="steve-cronan-production-metaverse.html" class="picto im-angle-left">Previous: The Production Metaverse</button>
          <!-- previousEnd -->
          <a id="alltalks" href="../talks.html" class="picto im-data">All talks</a>
          <!-- nextStart -->
          <button form="form" id="nexttalk" type="submit" formaction="julian-fernandez-campon-ipaas.html">Next: The Power of an iPaaS for Media<span class="picto im-angle-right"></span></button>
          <!-- nextEnd -->
        </p>
      </section>

      <section id="sponsors">
        <h2>
          Workshop sponsor
        </h2>
        <p><a href="https://www.adobe.com/"><img src="../media/adobe.png" alt="Adobe" width="70"></a></p>
        <p class="small">Interested in sponsoring the workshop?<br/>Please check the <a href="sponsors.html">sponsorship package</a>.</p>
      </section>
    </main>
    <footer class="footer" id="footer">
      <p>
        W3C is proud to be an open and inclusive organization, focused on
        productive discussions and actions. Our <a href=
        "https://www.w3.org/Consortium/cepc/">Code of Ethics and Professional
        Conduct</a> ensures that all voices can be heard.
      </p>
      <p>Questions? Contact FranÃ§ois Daoust
        &lt;<a href="mailto:fd@w3.org">fd@w3.org</a>&gt;.
      </p>
      <p>
        Suggestions for improving this workshop page, such as fixing typos or
        adding specific topics, can be made by opening a <a href=
        "https://github.com/w3c/media-production-workshop/">pull request on
        GitHub</a>, or by emailing FranÃ§ois Daoust
        &lt;<a href="mailto:fd@w3.org">fd@w3.org</a>&gt;.
      </p>
    </footer>
    <script src="../script.js"></script>
    <script>
      let captions = [
  {
    "language": "en",
    "label": "English",
    "src": "captions/bruce-devlin-metadata.vtt",
    "mode": "hidden",
    "cues": [],
    "activeCues": [
      {
        "text": ""
      }
    ]
  }
];
    </script>
    <script src="https://www.w3.org/2019/09/TPAC-template/parser.js"></script>
    <script src="https://embed.videodelivery.net/embed/sdk.latest.js"></script>
    <script src="../talk-sync.js"></script>
    <script src="https://w3c.github.io/i-slide/i-slide.js" type="module"></script>
  </body>
</html>
