<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Audio latency in browser-based DAWs - Ulf Hammarqvist - W3C/SMPTE Joint Workshop on Professional Media Production on the Web</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../style.css">
    <link rel="stylesheet" href="../talk-ui.css">
    <link rel="stylesheet" href="https://www.w3.org/2019/09/TPAC-template/fonts/iconmonstr-iconic-font.css">
    <meta name="twitter:site" content="@w3c">
    <meta name="twitter:card" content="player">
    <meta property="og:title" content="Audio latency in browser-based DAWs by Ulf Hammarqvist (Soundtrap/Spotify)">
    <meta property="og:description" content="Soundtrap implements a collaborative Digital Audio Workstation (DAW) in the browser and makes heavy  use of WebAudio and other web APIs. We highlight two needs for audio/midi recording use case:

  Low latency for &apos;monitoring&apos;. Ex, feeding the input through effects or playing a software instrument.
  Accurate latency information to perform &apos;latency compensation&apos; when recording multiple &apos;tracks&apos;.
">
    <meta property="og:image" content="thumbnails/ulf-hammarqvist-audio-latency.jpg">
    <meta property="og:video" content="https://iframe.videodelivery.net/9b3ce23ab13fd3e9fcbfefd01d01594b">
    <meta property="twitter:player" content="https://iframe.videodelivery.net/9b3ce23ab13fd3e9fcbfefd01d01594b?defaultTextTrack=en">
    <meta property="twitter:player:width" content="360">
    <meta property="twitter:player:height" content="202">
  </head>
  <body>
    <form id=form><!-- form to request a page in kiosk mode --></form>
    <header id="header" class="header">
      <div id="banner">
        <div>
          <p>
            <a href="https://www.w3.org/"><img alt="W3C" src=
            "../media/w3c_home_nb-v.svg" height="48" width="72"></a>
            <a href="https://www.smpte.org/"><img alt="SMPTE" src=
            "../media/smpte_logo.png" height="48"></a>
          </p>
          <div class="banner-title">
            <h1>
              W3C/SMPTE Joint Workshop on Professional Media Production on the Web
            </h1>
          </div>
          <p class="attribution">
            <span>Timeline photo by <a href="https://unsplash.com/@kineticbear?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Jacob Miller</a> on <a href="https://unsplash.com/s/photos/timeline?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>
          </p>
          <p>9-18 November 2021; online event</p>
        </div>
      </div>
      <nav class="menu" id="menu">
        <ul>
          <li>
            <a href="../">Call for Participation</a>
          </li>
          <li>
            <a href="../talks.html">Talks</a>
          </li>
          <li>
            <a href="../speakers.html">Apply as a speaker</a>
          </li>
        </ul>
      </nav>
    </header>
    <main id="main" class="main talk">
      <section id="intro">
        <h2>Audio latency in browser-based DAWs</h2>

        <p class="talkinfo">
          Presenter: <strong>Ulf Hammarqvist (Soundtrap/Spotify)</strong><br>
          Duration: <strong>10 minutes</strong><br>
          Slides: <a href="slides/ulf-hammarqvist-audio-latency.pdf"><strong>PDF</strong></a>
        </p>

        <p class="buttons">
          <!-- previousStart -->
          <button form="form" type="submit" class="picto im-angle-left" formaction="peter-salomonsen-webassembly-music.html">
            Previous: WebAssembly Music - latency/stability across platforms 
          </button>
          <!-- previousEnd -->
          <a href="../talks.html" class="picto im-data">All talks</a>
          <!-- nextStart -->
          <button form="form" type="submit" formaction="max-grosse-openexr.html">
            Next: Reviewing Production OpenEXR files on the Web for ML<span class="picto im-angle-right"></span>
          </button>
          <!-- nextEnd -->
        </p>
      </section>

      <section id="talk">
        <h3 style="display:none">Slides &amp; video</h3>

        <details>
          <summary>Keyboard shortcuts in the video player</summary>
          <ul>
            <li>Play/pause: <kbd>space</kbd>
            </li><li>Increase volume: <kbd>up arrow</kbd>
            </li><li>Decrease volume: <kbd>down arrow</kbd>
            </li><li>Seek forward: <kbd>right arrow</kbd>
            </li><li>Seek backward: <kbd>left arrow</kbd>
            </li><li>Captions on/off: <kbd>C</kbd>
            </li><li>Fullscreen on/off: <kbd>F</kbd>
            </li><li>Mute/unmute: <kbd>M</kbd>
            </li><li>Seek to 0%, 10%… 90%: <kbd>0-9</kbd>
          </li></ul>
        </details>

        <div id="player">
          <script type="application/ld+json">
            {
              "@context": "https://schema.org",
              "@type": "VideoObject",
              "name": "Audio latency in browser-based DAWs",
              "description": "Soundtrap implements a collaborative Digital Audio Workstation (DAW) in the browser and makes heavy  use of WebAudio and other web APIs. We highlight two needs for audio/midi recording use case:\n\n  Low latency for 'monitoring'. Ex, feeding the input through effects or playing a software instrument.\n  Accurate latency information to perform 'latency compensation' when recording multiple 'tracks'.\n",
              "thumbnailUrl": "thumbnails/ulf-hammarqvist-audio-latency.jpg",
              "duration": "PT10M3S",
              "embedUrl": "https://iframe.videodelivery.net/9b3ce23ab13fd3e9fcbfefd01d01594b",
            }
          </script>

          <iframe id="video" title="Audio latency in browser-based DAWs" src="https://iframe.videodelivery.net/9b3ce23ab13fd3e9fcbfefd01d01594b"
            allow="accelerometer; autoplay; encrypted-media; picture-in-picture"
            allowfullscreen="" width="640" height="360" frameborder="0"></iframe>

          <div id="slides" class="fade-in" role="region" aria-live="off" aria-label="Slide container">
            <i-slide id="ts-1" src="slides/ulf-hammarqvist-audio-latency.pdf#1" class="slide">Slide 1 of 12</i-slide>
<div>
  <p id="tp-1">Greetings from Sweden. This is Ulf over at Soundtrap from Spotify and I'm going to give a talk around some audio latency aspects of the web standards and the browser states.</p>
</div>

<i-slide id="ts-2" src="slides/ulf-hammarqvist-audio-latency.pdf#2" class="slide">Slide 2 of 12</i-slide>
<div>
  <p id="tp-2">So let's go to the slide number two directly and just watch a small clip showcasing using Soundtrap.</p>
  <p id="tp-3">(gentle music)</p>
  <p id="tp-4">(enthusiastic music)</p>
  <p id="tp-5">Okay. So, what you saw and heard was me messing about with the product itself, playing some notes, and playback of a project.</p>
</div>

<i-slide id="ts-3" src="slides/ulf-hammarqvist-audio-latency.pdf#3" class="slide">Slide 3 of 12</i-slide>
<div>
  <p id="tp-6">What is Soundtrap then?</p>
  <p id="tp-7">Well, Soundtrap is an online collaborative DAW and DAW is, as you may know, a Digital Audio Workstation.</p>
  <p id="tp-8">And, this means that we have features such as multi-track with a media recording and editing. We have software instruments, we have audio effects, reverbs, filters, guitar amp simulation and so on. And we implement all of this through heavy use of web standards such as Web Audio, Web media, MediaRecorder, MediaStream and so on and so forth</p>
</div>

<i-slide id="ts-4" src="slides/ulf-hammarqvist-audio-latency.pdf#4" class="slide">Slide 4 of 12</i-slide>
<div>
  <p id="tp-9">But as I mentioned that we focusing here today on aspects of audio latency, and it is essential to many of our use cases of Soundtrap.</p>
  <p id="tp-10">Specifically here, we want to mention the case of monitoring, and that is immediate feedback from what you record. So you would record from a mic, process stuff in Web Audio and play back through your speaker as you go.</p>
  <p id="tp-11">A good example of that is a guitarist that uses the DAW as a substitute really for the hardware, as pedals and amplifiers.</p>
  <p id="tp-12">And they play along with what they hear. I mean, it matters what the output, resulting output is, like, how hard should I string to get this kind of tone in my riff, or whatever.</p>
  <p id="tp-13">And another example is a keyboardist playing using the DAW as a software instrument essentially. And if there's a substantial lag from pressing a key to hearing it the note you get thrown off. Additionally you play along over an existing track or set of tracks.</p>
</div>

<i-slide id="ts-5" src="slides/ulf-hammarqvist-audio-latency.pdf#5" class="slide">Slide 5 of 12</i-slide>
<div>
  <p id="tp-14">Well, what is the current state? I mean, we can see something like 30ms best case round-trip latency, which is passable for monitoring purposes, but not great. In some cases it's not sufficiently low, we think and we'd like to get this much lower to be able to compete with the native offerings. Really.</p>
  <p id="tp-15">I mean, 10ms is a good target, that really is decent.</p>
</div>

<i-slide id="ts-6" src="slides/ulf-hammarqvist-audio-latency.pdf#6" class="slide">Slide 6 of 12</i-slide>
<div>
  <p id="tp-16">The second problem that we have in our use case that is maybe not immediately clear obvious to people is you record several things in succession and you need to align them. (chuckles) Sounds obvious, but it is trickier than you might think. That we call recording latency compensation. There are probably other words for it as well.</p>
  <p id="tp-17">In order to achieve that you need two things really, you need to know what is the round-trip latency. You need to know that actually, the actual latency. And you need to know when the data arrived towards your storage or stream, or what have you. To be able to when you play them back later, the data that you aggregated, the relative alignment of what you heard and what you produced should be retained.</p>
</div>

<i-slide id="ts-7" src="slides/ulf-hammarqvist-audio-latency.pdf#7" class="slide">Slide 7 of 12</i-slide>
<div>
  <p id="tp-18">And what do we mean by roundtrip latency? Well, that is several things combined. It's the input latency, it's the processing latency and it's the output latency. And that's not very surprising, I guess. But we'll get back to why it gets complicated.</p>
  <p id="tp-19">Then what if we have the wrong information or really no information, we can't do a good job, we would have misaligned playback, and obviously we do something, but that involves educated guesswork and that's not ideal. We'd like to explicitly know what the latencies are that are involved here.</p>
  <p id="tp-20">I mean, if not doing anything, the user would have to manually align, which is far from ideal.</p>
</div>

<i-slide id="ts-8" src="slides/ulf-hammarqvist-audio-latency.pdf#8" class="slide">Slide 8 of 12</i-slide>
<div>
  <p id="tp-21">The point is that both the input and output paths have many pieces or steps involved. And these things look differently on different audio stacks and/or operating systems and what have you.</p>
  <p id="tp-22">The latencies introduced by these steps are also vastly different, and not all of them, allow knowing this, so naturally strive for picking technical solutions that allow this. To allow knowing this, sorry, I should say.</p>
  <p id="tp-23">It's not just the path towards the browser or from the browser that may have hidden latencies. It could also be within the browser itself if you're not careful. And here we try to illustrate this with, well, bear with us. We pretend that the MediaStreamTrack setting latency property is a good indication of the input path latency. Might not be, but we can return to that.</p>
  <p id="tp-24">Anyway, so we operate in the Web Audio world. So that means that we take that number from the setting, and then we can see it probably isn't the same once you sort of stepped into the Web Audio world, because there's a node there as well, which may or may not do any additional buffering and that additional buffering or latency is not exposed anywhere.</p>
  <p id="tp-25">But it ought to be, otherwise we don't have good accurate numbers.</p>
</div>

<i-slide id="ts-9" src="slides/ulf-hammarqvist-audio-latency.pdf#9" class="slide">Slide 9 of 12</i-slide>
<div>
  <p id="tp-26">For completeness, I'll just talk a little bit about the output and the processing steps.</p>
  <p id="tp-27">And it has a certain quantum size or block size, or which is separate quantum sum. That latency also needs to be known and tracked somewhere.</p>
  <p id="tp-28">And likewise for the input, the output latencies also need to be exposed somehow somewhere.</p>
  <p id="tp-29">If you take a closer look at the Web Audio context property outputLatency, it does seem to indicate that is the block size as well as the output path combined, but it's not immediately clear.</p>
</div>

<i-slide id="ts-10" src="slides/ulf-hammarqvist-audio-latency.pdf#10" class="slide">Slide 10 of 12</i-slide>
<div>
  <p id="tp-30">The other aspect to get compensation done correctly is knowing accurately when your data arrived, as I mentioned earlier. It can be done in various ways, but not really. There's no brilliant way to do it.</p>
  <p id="tp-31">Traditionally, we've been using the MediaRecorder and it's nice in many ways, because that allows you to sort of encode on the fly and things like that.</p>
  <p id="tp-32">But there's no, as far as we can see spec-wise or guarantees that when you start it, it's going to start immediately. And even if it were you're still going to be like a quantum or two off your sample, but then it started, you get the idea.</p>
  <p id="tp-33">And I mean, a second option could be like, you do something custom based on a Worklet. You know everything then, but then you have to do everything as well. And even if using something like WebCodecs, it seems that you still have to packetize into like a container format, which is, I mean, it can be done, but it's not ideal.</p>
</div>

<i-slide id="ts-11" src="slides/ulf-hammarqvist-audio-latency.pdf#11" class="slide">Slide 11 of 12</i-slide>
<div>
  <p id="tp-34">To conclude this talk, first off the input and output latency are maybe spec-ed. It's not clear that they are the full paths that we intend here.</p>
  <p id="tp-35">And then specifically, we have some thoughts around the MediaStreamSourceNode and similar translation pieces between different standards, like any additional latency needs to be exposed.</p>
  <p id="tp-36">And then, there seems to be an opportunity to spice up the media stream recorder, to get accurate timing information by some callback or something.</p>
  <p id="tp-37">Finally, the WebCodecs API is very nice, but there's something missing in the sense that we don't have packetization, or containerization or whatever the correct term would be.</p>
  <p id="tp-38">And finally, of course, we just want to encourage all the implementers to get the input and output latencies low obviously, and pick drivers that allow exposing the information because we need it to do a good job.</p>
</div>

<i-slide id="ts-12" src="slides/ulf-hammarqvist-audio-latency.pdf#12" class="slide">Slide 12 of 12</i-slide>
<div>
  <p id="tp-39">Thank you. That's all for us. And see you at the discussions.</p>
</div>


          </div>
        </div>
      </section>

      <section id="extrabuttons">
        <p class="buttons">
          <!-- previousStart -->
          <button form="form" id="prevtalk" type="submit" formaction="peter-salomonsen-webassembly-music.html" class="picto im-angle-left">Previous: WebAssembly Music - latency/stability across platforms </button>
          <!-- previousEnd -->
          <a id="alltalks" href="../talks.html" class="picto im-data">All talks</a>
          <!-- nextStart -->
          <button form="form" id="nexttalk" type="submit" formaction="max-grosse-openexr.html">Next: Reviewing Production OpenEXR files on the Web for ML<span class="picto im-angle-right"></span></button>
          <!-- nextEnd -->
        </p>
      </section>

      <section id="sponsors">
        <h2>
          Workshop sponsor
        </h2>
        <p><a href="https://www.adobe.com/"><img src="../media/adobe.png" alt="Adobe" width="70"></a></p>
        <p class="small">Interested in sponsoring the workshop?<br/>Please check the <a href="sponsors.html">sponsorship package</a>.</p>
      </section>
    </main>
    <footer class="footer" id="footer">
      <p>
        W3C is proud to be an open and inclusive organization, focused on
        productive discussions and actions. Our <a href=
        "https://www.w3.org/Consortium/cepc/">Code of Ethics and Professional
        Conduct</a> ensures that all voices can be heard.
      </p>
      <p>Questions? Contact François Daoust
        &lt;<a href="mailto:fd@w3.org">fd@w3.org</a>&gt;.
      </p>
      <p>
        Suggestions for improving this workshop page, such as fixing typos or
        adding specific topics, can be made by opening a <a href=
        "https://github.com/w3c/media-production-workshop/">pull request on
        GitHub</a>, or by emailing François Daoust
        &lt;<a href="mailto:fd@w3.org">fd@w3.org</a>&gt;.
      </p>
    </footer>
    <script src="../script.js"></script>
    <script>
      let captions = [
  {
    "language": "en",
    "label": "English",
    "src": "captions/ulf-hammarqvist-audio-latency.vtt",
    "mode": "hidden",
    "cues": [],
    "activeCues": [
      {
        "text": ""
      }
    ]
  }
];
    </script>
    <script src="https://www.w3.org/2019/09/TPAC-template/parser.js"></script>
    <script src="https://embed.videodelivery.net/embed/sdk.latest.js"></script>
    <script src="../talk-sync.js"></script>
    <script src="https://w3c.github.io/i-slide/i-slide.js" type="module"></script>
  </body>
</html>
